{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze MCE data taken on TmVO4 samples\n",
    "either in DR (plate-like sample) or in He3 (needles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "# Standard Python libraries\n",
    "import csv, glob, os, re, sys, time\n",
    "from datetime import date\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from warnings import warn\n",
    "\n",
    "# Data analysis libraries\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import polyval2d\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "# Plotting libraries\n",
    "%matplotlib\n",
    "from matplotlib import pyplot as plt, cm, rcsetup, rc, rcParams\n",
    "from mplcursors import cursor\n",
    "# cm stands for colormap\n",
    "\n",
    "# Fitting libraries\n",
    "from lmfit import minimize, Model, Parameters, report_fit, fit_report\n",
    "from lmfit.models import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create today's date variables\n",
    "today = date.today()\n",
    "this_month = str(today)[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample parameters \n",
    "# He3 sample is TmVO4-LS5200-needles, DR sample is TmVO4-LS5228-DR-HC1807\n",
    "mfd_dir = {'LS5200':r'C:\\Users\\Pierre\\Desktop\\Postdoc\\Software\\COMSOL\\TmVO4-LS5200_HC2017-07\\TmVO4-LS5200_HC2017-07_COMSOL_results',\n",
    "           'LS5228':r'C:\\Users\\Pierre\\Desktop\\Postdoc\\Software\\COMSOL\\TmVO4-LS5228-DR-HC1807'}\n",
    "\n",
    "mfdFname = {'LS5200':['2020-11-03_TmVO4-LS5200_HC17-VII_T=p3-p4-3p1_Hext=all.csv'], \n",
    "            'LS5228':['2021-02-11_TmVO4-LS5228-DR-HC1807_Hz_Hc=5kOe_Hext=2-10kOe_T=p1-3p1K.csv']}\n",
    "\n",
    "# Choose which sample to work with\n",
    "sample = 'LS5228'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comsol magnetic field distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to directory containing MFD data\n",
    "os.chdir(mfd_dir[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.78143\n",
       "1         0.78143\n",
       "2         0.78143\n",
       "3         0.87819\n",
       "4         0.87819\n",
       "           ...   \n",
       "153779    0.84014\n",
       "153780    0.84014\n",
       "153781    0.84014\n",
       "153782    0.84014\n",
       "153783    0.84014\n",
       "Name: mfnc.Hz/Hext (1) @ 56: Hext=10000 Oe, T=3.1 K, Length: 153784, dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Import magnetic field distribution from CSV file\n",
    "# mfdNdlFname = ['2020-11-03_TmVO4-LS5200_HC17-VII_T=p3-p4-3p1_Hext=all.csv']\n",
    "for elmt in ['header', 'mfd_list', 'Hext_list', 'Tmfd_list']:\n",
    "    exec(f'{elmt} = []')\n",
    "mfd = {}\n",
    "\n",
    "for fname in mfdFname[sample]:\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for _ in range(9):\n",
    "            row = next(reader)  # gets the first line\n",
    "        for idx in range(len(row)):\n",
    "            if 'mfnc' in row[idx]:\n",
    "                header.append(','.join([row[idx], row[idx+1]]))\n",
    "                mh = re.match('.*@ (\\d+): (\\wext)=(\\d[\\.\\d]+) \\w+, T=(\\d\\.\\d) K', header[-1])\n",
    "                mfd_idx = int(mh.group(1))\n",
    "                if mh.group(2)=='Bext':\n",
    "                    Hext = float(mh.group(3))*1e4\n",
    "                elif mh.group(2)=='Hext':\n",
    "                    Hext = float(mh.group(3))\n",
    "                Tmfd = float(mh.group(4))\n",
    "                mfd_key = (Hext, Tmfd)\n",
    "                mfd[mfd_key] = {}\n",
    "                Hext_list.append(Hext)\n",
    "                Tmfd_list.append(Tmfd)\n",
    "                # Read MFD data from csv file:\n",
    "                # Use last string in header as column header\n",
    "                # Read data from column mfd_idx+2 since, the 3 first columns are x, y, z data\n",
    "                # squeeze=True in order to make it a Series instead of a DataFrame, since there is only one column for each mfd_idx\n",
    "                mfd[mfd_key]['data'] = pd.read_csv(fname, comment='%', \n",
    "                                                   names=[header[-1]], \n",
    "                                                   usecols=[mfd_idx+2],\n",
    "                                                   squeeze=True)\n",
    "                # If all elements of an MFD are NaN, delete the corresponding entry\n",
    "                if mfd[mfd_key]['data'].isna().all():\n",
    "                    del mfd[mfd_key]\n",
    "            # elif and else are only for debugging purposes\n",
    "            elif 'T=' in row[idx]: continue\n",
    "            else: header.append(row[idx])\n",
    "uhext = np.unique(Hext_list)\n",
    "utmfd = np.unique(Tmfd_list)\n",
    "mfd[mfd_key]['data']# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ee4e3c0608>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(mfd[mfd_key]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find relative width of a distribution\n",
    "def mfd_rel_width(mfd_data, Href):\n",
    "    # purge distribution from non-zero values \n",
    "    # useful for datasets computed in a box bigger than Comsol model\n",
    "    # and to ignore NaN data\n",
    "    real_data = mfd_data[mfd_data>0]/Href# normalize wrt Hc or Hext\n",
    "\n",
    "    # Find relative value of mfd that is furthest away from 1\n",
    "    if abs(1-min(real_data))>abs(1-max(real_data)): \n",
    "        relative_width = abs(1-min(real_data))\n",
    "    else: \n",
    "        relative_width = abs(1-max(real_data))\n",
    "\n",
    "    return relative_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create histogram from MFD data\n",
    "def mfd_histogram(mfd_data, Href, nbins=75):\n",
    "    # purge distribution from non-zero values \n",
    "    # useful for datasets computed in a box bigger than Comsol model\n",
    "    # and to ignore NaN data\n",
    "    real_data = mfd_data[mfd_data>0]/Href# normalize wrt Hc or Hext\n",
    "\n",
    "    # Find relative value of mfd that is furthest away from 1, \n",
    "    # so as to define the histogram range base on this value\n",
    "    # This allows to create a symmetric histogram, which is useful for later convolution\n",
    "    half_range = mfd_rel_width(mfd_data, Href)\n",
    "        \n",
    "    # compute histogram of distribution\n",
    "    hist_counts, edges = np.histogram(real_data, bins=nbins, \n",
    "                                      range=(1-half_range, 1+half_range),\n",
    "                                      density=True)\n",
    "    bin_centers = np.mean([edges[:-1], edges[1:]], 0)\n",
    "    bin_widths = edges[1:]-edges[:-1]\n",
    "\n",
    "    return hist_counts, bin_centers, bin_widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Compute probability distribution of fields at a given value of T and Hext\n",
    "for key in mfd.keys():\n",
    "    if sample=='LS5200':\n",
    "        Hext = key[0]\n",
    "    elif sample=='LS5228':\n",
    "        Hext = 1\n",
    "#     Tmfd = key[1]\n",
    "    # Compute mfd histogram and unpack it into into dictionary key\n",
    "    mfd[key]['rel_width'] = mfd_rel_width(mfd[key]['data'], Hext)\n",
    "    mfd[key]['hc'], mfd[key]['binCenters'], mfd[key]['binWidths'] = \\\n",
    "    mfd_histogram(mfd[key]['data'], Hext, nbins=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mplcursors._mplcursors.Cursor at 0x1ee50e8c108>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Plot distribution of fields at a given value of T and Hext\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt_keys = {'fixed T': [(uhext[i], utmfd[2])  for i in [0, 2, 4, 6]],\n",
    "           'fixed H': [(uhext[2], utmfd[i]) for i in [0, 2, 3, 5]]}\n",
    "# param_index = 1# 0 is constant T, 1 is constant Hext, see param_range\n",
    "# param_range = [, [i+4*8 for i in [1,3,5,7]]]# first range corresponds to a \n",
    "# # field dependence at constant temp, second range corresponds to a \n",
    "# # temperature dependence at constant field\n",
    "# # rng = param_range[param_index]#\n",
    "Hc = 5e3\n",
    "Tc = 2.2 \n",
    "setting = 'fixed H'\n",
    "\n",
    "if setting=='fixed T':\n",
    "    fixedp = Tmfd = plt_keys[setting][0][1]\n",
    "    varp_idx = 0\n",
    "    pc = Hc\n",
    "    lgd_title = '$H_{\\mathrm{ext}}/H_c$'\n",
    "    ann_str = f'$T/T_c=$ {Tmfd/Tc:.2g}'\n",
    "elif setting=='fixed H':\n",
    "    fixedp = Hext = plt_keys[setting][0][0]\n",
    "    varp_idx = 1\n",
    "    pc = Tc\n",
    "    lgd_title = '$T/T_c$'\n",
    "    ann_str = ''.join('$H_{\\mathrm{ext}} =$' f' {Hext/1e3:.3g} kOe')\n",
    "\n",
    "for key in plt_keys[setting]:\n",
    "#     if param_index==0: lgd_str = f'{Hext/Hc:.2g}'\n",
    "#     elif param_index==1: lgd_str = f'{Tndl/Tc:.2g}'\n",
    "    lgd_str = f'{key[varp_idx]/pc:.2g}'\n",
    "    p = plt.plot(mfd[key]['binCenters'], mfd[key]['hc'], '.-', label=lgd_str)\n",
    "# plt.xlim([.6966,1.05])\n",
    "\n",
    "lgd = plt.legend(title=lgd_title) \n",
    "anndist = plt.annotate(ann_str, xy=(.35, .85), xycoords='axes fraction', \n",
    "                       bbox=dict(boxstyle='round', fc='w'))# add annotation\n",
    "# needle_title = 'Field distribution in TmVO$_4$ needles'\n",
    "# param_title = [f', $T=$ {Tndl:%.2g} K', f', $H_{\\mathrm{ext}}=$ {Hext:%.2d} Oe']\n",
    "# title(''.join([needle_title, param_title[param_index]])\n",
    "plt.xlabel('$H_{\\mathrm{in}}/H_{\\mathrm{ext}}$')\n",
    "plt.ylabel('Probability density')\n",
    "# plt.xticks = np.arange(.7, 1, .1)\n",
    "plt.grid(b=True)\n",
    "cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Compute the average value of ratio of internal to external magnetic field \n",
    "for key in mfd.keys():\n",
    "    mfd[key]['Hinm_Oe'] = np.round(\n",
    "        np.sum(\n",
    "        mfd[key]['binCenters']*mfd[key]['hc']*mfd[key]['binWidths']\n",
    "        )*key[0]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0134"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Check results for given range of temperatures/field\n",
    "\n",
    "# Check that the ratio of Hinm/Hext is roughly constant inside the\n",
    "# ordered phase, thus allowing to use the value at 0.3 K and 1000 Oe as a proxy\n",
    "for elmt in ['h', 'Hinm', 'Hext', 'Tmfd']:\n",
    "    exec(f'{elmt}=[]')# initialize lists\n",
    "    \n",
    "for key in plt_keys[setting]:\n",
    "    Tmfd.append(key[1])\n",
    "    Hext.append(key[0])\n",
    "    Hinm.append(mfd[key]['Hinm_Oe'])\n",
    "    h.append(mfd[key]['Hinm_Oe']/key[0])\n",
    "\n",
    "# rescaling factor, due to demag; take value at H~Hc and T~Tc/2, which is where the MCE data is interesting\n",
    "rescaling = mfd[list(mfd)[-6]]['Hinm_Oe']/list(mfd)[-6][0]\n",
    "rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation of MCE data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "datFilePath = {'LS5200':r'C:\\Users\\Pierre\\Desktop\\Postdoc\\TmVO4\\TmVO4_heat-capacity\\2017-07_TmVO4_Cp_MCE\\2017-07-28--31\\Massaic_MCE',\n",
    "               'LS5228':r'C:\\Users\\Pierre\\Desktop\\Postdoc\\TmVO4\\TmVO4_heat-capacity\\2018-08_TmVO4-LS5228'}\n",
    "os.chdir(datFilePath[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries that will contain data for each run\n",
    "for key in ['dataset', 'run_date', 'I', 'R', 'H', 'T', 'Tpuck', \\\n",
    "            'Hi', 'Ti', 'Tpucki', 'dHi','dH', 'dT', 'd1T', 'd2T',\\\n",
    "            'mce_sim', 'mce_fit', 'refsweep', 'usr', 'utbath', 'sweeprates']:\n",
    "    if key not in locals():\n",
    "        exec(f'{key}={{}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-2fd7e421d10d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrunIDs_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrunID\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfile_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Run{runID}_*.dat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Run(\\d)_(0p\\d).*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mrunIDs_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# int(m.group(1)) also works\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "runIDs_all = []\n",
    "for runID in range(2,8):\n",
    "    file_str = glob.glob(f'Run{runID}_*.dat')[0]\n",
    "    m = re.match('Run(\\d)_(0p\\d).*', file_str)\n",
    "    runIDs_all.append(runID)# int(m.group(1)) also works\n",
    "    dataset[runID] = pd.read_csv(file_str, sep=', ', engine='python')\n",
    "    I[runID] = float(m.group(2).replace('p','.'))*1e-6# input current, in amps\n",
    "\n",
    "print(dataset[runID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[2].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of MCE resistance data to temperature \n",
    "using fits of 1/T vs R curves across values of magnetic field and input current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing betamodel.mat\n",
    "Which is the surface fit used in Matlab to convert the platform resistance data of the lockin to inverse temperature data:\n",
    "$$ 1/T = \\mathrm{betamodel}(V_X) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The content of this cell was copy-pasted from the content of the \n",
    "# BetaModel sfit variable, as shown in Matlab after importing betamodel.mat\n",
    "\n",
    "# Coefficients (with 95% confidence bounds):\n",
    "p00 =     -0.2734#  (-0.2752, -0.2716)\n",
    "p10 =    9.78e-06#  (8.802e-06, 1.076e-05)\n",
    "p01 =      0.0011#  (0.001096, 0.001104)\n",
    "p20 =   -4.45e-09#  (-5.037e-09, -3.862e-09)\n",
    "p11 =  -1.322e-08#  (-1.427e-08, -1.216e-08)\n",
    "p02 =  -2.275e-07#  (-2.305e-07, -2.246e-07)\n",
    "p30 =   9.561e-13#  (7.765e-13, 1.136e-12)\n",
    "p21 =   3.565e-12#  (3.342e-12, 3.789e-12)\n",
    "p12 =   4.659e-12#  (4.225e-12, 5.092e-12)\n",
    "p03 =    3.37e-11#  (3.272e-11, 3.468e-11)\n",
    "p40 =  -8.502e-17#  (-1.093e-16, -6.072e-17)\n",
    "p31 =  -6.191e-16#  (-6.464e-16, -5.918e-16)\n",
    "p22 =   8.508e-17#  (4.041e-17, 1.298e-16)\n",
    "p13 =  -6.088e-16#  (-6.86e-16, -5.316e-16)\n",
    "p04 =  -2.724e-15#  (-2.872e-15, -2.577e-15)\n",
    "p50 =   2.293e-21#  (1.105e-21, 3.48e-21)\n",
    "p41 =   3.892e-20#  (3.749e-20, 4.034e-20)\n",
    "p32 =   -1.77e-20#  (-1.973e-20, -1.567e-20)\n",
    "p23 =   1.698e-21#  (-1.673e-21, 5.069e-21)\n",
    "p14 =   2.632e-20#  (2.116e-20, 3.148e-20)\n",
    "p05 =   8.741e-20#  (7.917e-20, 9.565e-20)\n",
    "\n",
    "# Linear model Poly55:\n",
    "def betamodel(x,y):\n",
    "    \"\"\"\n",
    "    x: array\n",
    "        Magnetic field data.\n",
    "    y: array\n",
    "        Platform resistance data.\n",
    "    \"\"\"\n",
    "    val = p00 + p10*x + p01*y + p20*x**2 + p11*x*y + p02*y**2 + p30*x**3 +\\\n",
    "    p21*x**2*y + p12*x*y**2 + p03*y**3 + p40*x**4 + p31*x**3*y \\\n",
    "    + p22*x**2*y**2 + p13*x*y**3 + p04*y**4 + p50*x**5 + p41*x**4*y \\\n",
    "    + p32*x**3*y**2 + p23*x**2*y**3 + p14*x*y**4 + p05*y**5\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved R-T conversion scheme\n",
    "Instead of fitting resistance data stored in the .raw files of Cp measurements, which is what the betamodel function did, fit resistance data directly from the calibration file. This is more accurate for two reasons:\n",
    "1. There are way more datapoints in the calibration file then in the .raw files of Cp measurements, and they are not redundent\n",
    "2. The .raw files of Cp measurements use an approximation of the R vs T data taken from the calibration file. Fitting data from the .raw files therefore introduces an extra step (with an extra approximation) in the conversion of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Pierre\\Desktop\\Postdoc\\Technical_stuff\\PPMS\\PPMS_Heat_capacity\\PPMS_Heat-capacity_TempCal')\n",
    "\n",
    "calname = {'LS5200':'./He3_TempCal/2021-01-15_He3Puck698_multicurve_all_currents.dat',\n",
    "           'LS5228':'./DR_TempCal/2021-02-02_DR_Tempcal_FisherPPMS/2021-02-02_DRPuck167_multicurve_all_currents.dat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MFD data from csv file:\n",
    "# Use last string in header as column header\n",
    "# Read data from column mfd_idx+2 since, the 3 first columns are x, y, z data\n",
    "# squeeze=True in order to make it a Series instead of a DataFrame, since there is only one column for each mfd_idx\n",
    "cal = pd.read_csv(calname[sample], skiprows=7, header=0)\n",
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = cal.iloc[:,:5]\n",
    "# del cal['Active Current Code']\n",
    "for cname in cal.columns:\n",
    "    cal.rename(columns={cname:cname.strip(' ')}, inplace=True)\n",
    "cal.sort_values('Current (code)', inplace=True, ignore_index=True)\n",
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codename = './2021-01-19_DSPCurrentCodes.txt'\n",
    "Icodes = pd.read_csv(codename, sep=' ', names=['code', 'current', 'unit'])\n",
    "Icodes['I (uA)'] = np.where(Icodes['unit']=='nA', Icodes['current']*1e-3,\n",
    "                            np.where(Icodes['unit']=='mA', Icodes['current']*1e3, \n",
    "                                     Icodes['current']))\n",
    "Icodes.sort_values('code', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcal = {}\n",
    "for code in np.unique(cal['Current (code)']):\n",
    "    dcal[code] = np.where(cal['Current (code)']==code, Icodes['I (uA)'][Icodes['code']==code].item(), 0)\n",
    "    \n",
    "cal['I (uA)'] = np.concatenate([dcal[i][dcal[i]>0] for i in np.unique(cal['Current (code)'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal[cal['Current (code)']==11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxT = 2\n",
    "subcal = cal[cal['Temperature (K)']<maxT]\n",
    "xi = subcal['I (uA)']# current\n",
    "xm = subcal['Field (Oe)']# magnetic field\n",
    "z = subcal['Temperature (K)']# temperature\n",
    "xr = subcal['ThRes (ohms)']# resistance\n",
    "Xi = np.unique(xi)\n",
    "Xm = np.arange(0, 1.1e4, 1e2)#\n",
    "# Xt = np.arange(min(y), maxT, .01)\n",
    "Xr = np.arange(7e2, 1.0e4, 1e2)#np.unique(np.round(xr,-1))\n",
    "# XI, XM, XT = np.meshgrid(Xi, Xm, Xt)  # 3D grid for interpolation\n",
    "X, Y = np.meshgrid(Xm, Xr)# grid for interpolation\n",
    "# interp = LinearNDInterpolator(list(zip(xi, xm, xt)), z)\n",
    "# interp = LinearNDInterpolator(list(zip(xm, xr)), z)\n",
    "# Z = interp(XI, XM, XT)\n",
    "# Z = interp(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2dcoeffs(pars):\n",
    "    last = list(pars.keys())[-1]# parameter with highest indices\n",
    "    kx, ky = int(last[1]), int(last[2])# order of 2d polynomial\n",
    "    coeffs = np.empty((kx+1,ky+1))# initialize array of coefficients\n",
    "\n",
    "    parvals = pars.valuesdict()\n",
    "    for paridx, (i,j) in enumerate(np.ndindex(kx+1,ky+1)):\n",
    "        coeffs[i,j] = list(parvals.values())[paridx]\n",
    "    \n",
    "    return coeffs\n",
    "\n",
    "def poly2dres(pars, x, y, z):\n",
    "    return polyval2d(x, y, poly2dcoeffs(pars))-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Parameters()\n",
    "for (i,j) in np.ndindex(5,5):\n",
    "    params.add(name=f'c{i}{j}', value=1)\n",
    "\n",
    "result = minimize(poly2dres, params, args=(xm, xr, 1/z))\n",
    "result.redchi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = polyval2d(X, Y, poly2dcoeffs(result.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(X, Y, 1/Z, c='k', marker='o')\n",
    "surf = ax.plot_surface(X, Y, 1/Z, cmap=cm.coolwarm, rcount=100,\n",
    "                       linewidth=0, antialiased=False)\n",
    "# surf = ax.contour3D(X, Y, 1/Z, 50, cmap='viridis')\n",
    "# Add a color bar which maps values to colors.\n",
    "cb = fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "# plt.xlim(right=1e4)\n",
    "plt.xlabel(xm.name)\n",
    "plt.ylabel(xr.name)\n",
    "cb.set_label(z.name)\n",
    "plt.title('Surface fit of T vs R, H data (circles)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert and store MCE data into dictionaries\n",
    "Resistance data obtained from the voltage data $V_X$ of the X channel of the lock-in amplifier and the input electrical current $I$ as:\n",
    "$$ R_\\mathrm{platform} = V_X / I $$\n",
    "Temperature data obtained from resistance data using the above polynomial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_mce = '$H$ (Oe)'\n",
    "ylabel_mce = '$T + \\Delta T_\\mathrm{MCE}$ (K)'\n",
    "# title_mce = f'MCE traces run {runID} - {run_date[runID]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runID in runIDs_all:\n",
    "    Hi[runID] = {'raw':dataset[runID]['Field(T)']*1e4} # Magnetic field, in Oersted\n",
    "    Hi[runID]['raw'].rename('Field (Oe)', inplace=True)\n",
    "    Tpucki[runID] = {'raw':dataset[runID]['Temperature(K)']}# Puck (bath) temperature, in Kelvin\n",
    "    R[runID] = dataset[runID]['LockinX(V)']/I[runID]# Platform resistance, in Volts\n",
    "    run_date[runID] = re.match('.* (\\d+/\\d+/20\\d{2}) .*', dataset[runID].columns[5]).group(1)\n",
    "#     Ti[runID] = {'raw':1/betamodel(Hi[runID]['raw'], R[runID])}\n",
    "    Ti[runID] = {'raw':1/polyval2d(Hi[runID]['raw'], R[runID], poly2dcoeffs(result.params))}\n",
    "#     temp = np.empty(R[runID].shape)\n",
    "#     for ridx, rval in enumerate(R[runID]):\n",
    "#         hcol = np.argmin(abs(Hi[runID]['raw'].iloc[ridx]-Xm))\n",
    "#         rrow = np.argmin(abs(rval-Xr))\n",
    "#         temp[ridx] = Z[rrow, hcol]\n",
    "#     Ti[runID] = {'raw':pd.Series(temp)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw MCE traces\n",
    "# runID = 6\n",
    "# %matplotlib \n",
    "pltIDs = [2]\n",
    "for runID in pltIDs:#runIDs_all[:]:\n",
    "    plt.figure(num=runID*10)\n",
    "#     plt.plot(Hi[runID]['raw'], Ti[runID]['raw'])\n",
    "    plt.plot(Hi[runID]['raw'], Ti[runID]['raw'])\n",
    "    plt.ylim(bottom=0.4)\n",
    "    plt.xlabel(xlabel_mce)\n",
    "    plt.ylabel(ylabel_mce)\n",
    "    plt.title(f'Raw MCE traces run #{runID} - {run_date[runID]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw MCE traces\n",
    "# runID = 6\n",
    "# %matplotlib \n",
    "pltIDs = [2]\n",
    "for runID in pltIDs:#runIDs_all[:]:\n",
    "    plt.figure(num=runID*10+1)\n",
    "#     plt.plot(Hi[runID]['raw'], Ti[runID]['raw'])\n",
    "    plt.plot(Hi[runID]['raw'], R[runID])\n",
    "    plt.ylim(bottom=0.4)\n",
    "    plt.xlabel(xlabel_mce)\n",
    "    plt.ylabel(xr.name)\n",
    "    plt.title(f'Raw MCE traces run #{runID} - {run_date[runID]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runID in runIDs_all:\n",
    "    for el in [Hi, Ti]:\n",
    "        el[runID]['smoothed'] = gaussian_filter1d(el[runID]['raw'], 1)\n",
    "        el[runID]['sssmooth'] = gaussian_filter1d(el[runID]['raw'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCE data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth MCE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed MCE traces\n",
    "pltIDs = [2]\n",
    "for runID in pltIDs:#runIDs_all:\n",
    "    plt.figure(num=runID+10)\n",
    "    labels = ['raw', 'smoothed', 'sssmooth']\n",
    "    for lbl in labels:\n",
    "        plt.plot(Hi[runID][lbl], Ti[runID][lbl], label=lbl)\n",
    "    plt.ylim(bottom=0.4)\n",
    "    plt.xlabel(xlabel_mce)\n",
    "    plt.ylabel(ylabel_mce)\n",
    "    plt.title(f'Smoothed MCE traces run #{runID} - {run_date[runID]}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dH data and extract sweeprates from it\n",
    "for runID in runIDs_all:\n",
    "    dHi[runID] = np.diff(Hi[runID]['smoothed'])\n",
    "#     dT[runID] = pd.Series(np.diff(Ti[runID]['smoothed']))\n",
    "#     d1T[runID] = dT[runID][dH[runID]>0]/dH[runID][dH[runID]>0]\n",
    "    udh, cdh = np.unique(np.round(dHi[runID]), return_counts=True)\n",
    "    sweeprates[runID] = udh[np.logical_and(cdh>250, udh!=0)]# 250 seems to be the best value to eliminate datapoints that are not actual traces\n",
    "    usr[runID], csr = np.unique(abs(sweeprates[runID]), return_counts=True)\n",
    "\n",
    "dHi[runID]#[d1T[runID].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataError(Exception):\n",
    "    \"\"\"Customized errors for the purpose of MCE data analysis.\"\"\"\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate traces based on sweeprate\n",
    "def sep_sweeprates(sweeprates, dH, X):\n",
    "    Xout = {}\n",
    "    \n",
    "    if len(sweeprates)<1:\n",
    "        raise DataError(\"No value of sweeprate was stored for this run.\")\n",
    "    elif len(sweeprates)==1:\n",
    "        sr = sweeprates[0]\n",
    "        Xout[sr] = pd.Series(X)\n",
    "    else:\n",
    "        for ir, sr in enumerate(sweeprates):\n",
    "            if ir==0:\n",
    "                refsweep = np.mean([sr, sweeprates[ir+1]])\n",
    "                out = X[dH<refsweep]\n",
    "            elif ir<len(sweeprates)-1:\n",
    "                rsm = np.mean([sr, sweeprates[ir-1]])\n",
    "                rsp = np.mean([sr, sweeprates[ir+1]])\n",
    "                out = X[np.logical_and(dH<rsp, dH>rsm)]\n",
    "            else:\n",
    "                refsweep = np.mean([sr, sweeprates[ir-1]])\n",
    "                out = X[dH>refsweep]\n",
    "            Xout[sr] = pd.Series(out)\n",
    "\n",
    "    return Xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute H, T, Tpuck\n",
    "for runID in runIDs_all:\n",
    "    for pq in [(Hi, H), (Ti, T), (Tpucki, Tpuck)]:# pq stands for physical quantity\n",
    "#         # Initialize dictionaries to store data\n",
    "#         if runID not in pq[0].keys():\n",
    "#             pq[0][runID] = {}\n",
    "#         if 'dict' not in pq[0][runID].keys():\n",
    "        pq[0][runID]['dict'] = {}# create/reset dictionary\n",
    "        pq[1][runID] = pd.DataFrame()# create/reset dataFrame\n",
    "            \n",
    "        # Reduce length of reference arrays by 1, since dH has 1 element less than these\n",
    "        try:\n",
    "            Xm = np.mean([pq[0][runID]['smoothed'][:-1], pq[0][runID]['smoothed'][1:]], 0)\n",
    "        except KeyError:\n",
    "            Xm = np.mean([pq[0][runID]['raw'][:-1], pq[0][runID]['raw'][1:]], 0)\n",
    "        \n",
    "        pq[0][runID]['dict'] = sep_sweeprates(sweeprates[runID], dHi[runID], Xm)\n",
    "\n",
    "        # Concatenate separated data into a single DataFrame, for each run\n",
    "        try:\n",
    "            pq[1][runID] = pd.concat(pq[0][runID]['dict'], axis=1)# 0 was chosen as key for simplicity\n",
    "        except ValueError:\n",
    "            warn(f\"Concatenation failed for data of run #{runID}.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dH, dT\n",
    "for runID in runIDs_all:\n",
    "    for el in [Hi, Ti]:\n",
    "        el[runID]['sssmooth'] = gaussian_filter1d(el[runID]['raw'], 50/max(abs(sweeprates[runID])))\n",
    "\n",
    "    dTm = np.diff(Ti[runID]['sssmooth'])\n",
    "    dHm = np.diff(Hi[runID]['sssmooth'])#dHi[runID]\n",
    "    dTd = sep_sweeprates(sweeprates[runID], dHi[runID], dTm)\n",
    "    dHd = sep_sweeprates(sweeprates[runID], dHi[runID], dHm)\n",
    "\n",
    "    for pq in [(dHd, dH), (dTd, dT)]:# pq stands for physical quantity\n",
    "        pq[1][runID] = pd.DataFrame()# create/reset dataFrame\n",
    "        # Concatenate separated data into a single DataFrame, for each run\n",
    "        try:\n",
    "            pq[1][runID] = pd.concat(pq[0], axis=1)# 0 was chosen as key for simplicity\n",
    "        except ValueError:\n",
    "            warn(f\"Concatenation failed for data of run #{runID}.\")\n",
    "            break\n",
    "            \n",
    "    dHfilter = abs(dH[runID])>0\n",
    "    d1T[runID] = dT[runID][dHfilter]/dH[runID][dHfilter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused as of 2021-01-05; delete if not used after that\n",
    "# dHm = np.mean([dH[runID][:-1], dH[runID][1:]], 0)\n",
    "# d2Tm = np.diff(dT[runID])\n",
    "# dHfilter = abs(dHm)>0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "for runID in runIDs_all:\n",
    "    fig = plt.figure()\n",
    "    for i, sr in enumerate(H[runID].columns):\n",
    "        plt.plot(H[runID][sr], T[runID][sr], '.', label=f'{sr:+.1f} Oe/s')\n",
    "    plt.ylim(bottom=0.4)\n",
    "    plt.xlabel(xlabel_mce)\n",
    "    plt.ylabel(ylabel_mce)\n",
    "    plt.title(f'MCE traces run #{runID} - {run_date[runID]}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\Pierre\\Desktop\\Postdoc\\Software\\Python\\Python_scripts')\n",
    "import MCE_simulations.mce_no_demag as mce\n",
    "# mce = reload(mce)# if mce is changed\n",
    "\n",
    "# Create generator of runIDs, excluding run #6 which is bad\n",
    "runIDs = [x for x in runIDs_all if x!=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify values of bath temperature used for the measurements\n",
    "npoints = 250\n",
    "for runID in runIDs:\n",
    "    # ut, ct = np.unique(np.round(Tpucki[runID]['raw'], 2), return_counts=True)# Use raw data\n",
    "    ut, ct = np.unique(np.round(Tpuck[runID], 1), return_counts=True)# Same, by construction of Tpuck[runID]\n",
    "\n",
    "    # Show counts for each value of bath temperature, ignoring NaN\n",
    "    print(runID, [[ut[ct>npoints][i], ct[ct>npoints][i]] for i in range(len(ut[ct>npoints]))])\n",
    "    utbath[runID] = ut[ct>npoints]# select only those values that occur in more than 500 data points\n",
    "utbath[runID]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bath_temp function\n",
    "sr = sweeprates[runID][0]\n",
    "for Tb in utbath[runID][:1]:\n",
    "    dataFilter = np.logical_and(np.logical_and(H[runID][sr]>10, H[runID][sr]<4e3),\n",
    "                                np.round(Tpuck[runID][sr], 1)==Tb)\n",
    "    # print(min(dTp), max(dTp))\n",
    "    Hp = H[runID][sr][dataFilter]\n",
    "    Tp = T[runID][sr][dataFilter]\n",
    "    Tb = mce.bath_temp([Tp], rel_temp_bound=1e-3, timeit=True)\n",
    "    print(Tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For given field and temperature, find two closest mfd's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_mfd_values(ptest, upmfd):\n",
    "    # sort array of fields by distance to current value of field and keep the two closest values\n",
    "    dpmfd = abs(upmfd-ptest)# \n",
    "    closest2p = upmfd[np.argsort(dpmfd)][:2]\n",
    "    pweights = 1 - dpmfd[np.argsort(dpmfd)][:2]/abs(np.diff(closest2p)[0])\n",
    "\n",
    "    # if the value of field is below the lowest value in the array or above the highest, \n",
    "    # i.e. if the distance between the former and the second closest value in the array is larger than \n",
    "    # the distance between the two closest values in the array\n",
    "    if dpmfd[np.argsort(dpmfd)][1]>=abs(np.diff(closest2p)[0]):\n",
    "        # only keep the single closest value in the array (i.e. the lowest or the highest)\n",
    "        closest2p = closest2p[:1]\n",
    "        pweights = np.ones(1)\n",
    "        # for instance: if H = 345 and the array is [1000, 2000, 3000], only keep 1000,\n",
    "        # since abs(2000-345)>abs(2000-1000)\n",
    "    \n",
    "    return closest2p, pweights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute second derivative of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "pltIDs = [2]\n",
    "sr = -10\n",
    "runID = 2\n",
    "d2Ttest = {}\n",
    "d2Ttest[sr] = pd.Series(gaussian_filter1d(d1T[runID][sr], 10))\n",
    "for runID in pltIDs:#runIDs:\n",
    "    plt.figure()\n",
    "    plt.plot(H[runID][sr], d1T[runID][sr])\n",
    "    plt.plot(H[runID][sr], d2Ttest[sr].diff()/dH[runID][sr]*1e2)\n",
    "#     plt.ylim(bottom=0.4)\n",
    "    plt.xlabel(xlabel_mce)\n",
    "    plt.ylabel(ylabel_mce)\n",
    "    plt.title(f'Smoothed MCE traces run #{runID} - {run_date[runID]}')\n",
    "    plt.ylim(np.arange(-1, 2, 2)*1e-4)\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elmt in ['d2T', 'd2Tdf', 'HTcd1', 'HTcd2', 'dHc']:\n",
    "    exec(f'{elmt} = {{}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filter(runID, sr, Tb):\n",
    "    return np.logical_and(np.logical_and(H[runID][sr]>4e3, H[runID][sr]<5.75e3),\n",
    "                                         np.round(Tpuck[runID][sr], 1)==Tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runID in runIDs[:]:\n",
    "    for elmt in ['d2T', 'd2Tdf', 'HTcd1', 'HTcd2', 'dHc']:\n",
    "        exec(f'{elmt}[runID] = {{}}')\n",
    "    for sr in sweeprates[runID][:]:\n",
    "        for elmt in ['d2Tdf', 'HTcd1', 'HTcd2', 'dHc']:\n",
    "            exec(f'{elmt}[runID][sr] = {{}}')\n",
    "\n",
    "        Nsmth = 50/abs(sr)\n",
    "        d1Tgauss = pd.Series(gaussian_filter1d(d1T[runID][sr], Nsmth))\n",
    "        d2T[runID][sr] = d1Tgauss.diff()/dH[runID][sr]\n",
    "        \n",
    "        for Tb in utbath[runID][:]:\n",
    "            if Tb==0.4: continue\n",
    "\n",
    "            dataFilter = np.logical_and(np.logical_and(H[runID][sr]>4e3, H[runID][sr]<5.75e3),\n",
    "                                        np.round(Tpuck[runID][sr], 1)==Tb)\n",
    "            Hf = H[runID][sr][dataFilter]\n",
    "            Tf = T[runID][sr][dataFilter]\n",
    "            d1Tf = d1T[runID][sr][dataFilter]\n",
    "            d2Tf = d2T[runID][sr][dataFilter]\n",
    "            \n",
    "            try:\n",
    "                # Save values of critical field; \n",
    "                # it seems that .idxmax() is systematically shifted by 1 index \n",
    "                # for the 2nd derivative, hence the .idxmax()-1\n",
    "                HTcd1[runID][sr][Tb] = (Hf[abs(d1Tf).idxmax()], Tf[abs(d1Tf).idxmax()])\n",
    "                HTcd2[runID][sr][Tb] = (Hf[d2Tf.idxmax()-1], Tf[d2Tf.idxmax()-1])\n",
    "                dHc[runID][sr][Tb] = Nsmth*abs(sr)*np.sqrt(2*np.log(2))\n",
    "            except (ValueError, KeyError):\n",
    "                continue\n",
    "\n",
    "#         print(HTcd1[runID][sr], HTcd2[runID][sr])\n",
    "#     d2Tdf[runID] = pd.DataFrame(d2T[runID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual corrections to computational errors\n",
    "for sr in [-14, 14]:\n",
    "    try:\n",
    "        del HTcd1[7][sr]\n",
    "        del HTcd2[7][sr]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "runID, sr, Tb = 7, 28, .5\n",
    "HTcd1[runID][sr][Tb] = (H[runID][sr].loc[535], T[runID][sr].loc[535])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare saving figure & data\n",
    "os.chdir(datFilePath[sample])\n",
    "savedir = {}\n",
    "for runID in runIDs:#runIDs:\n",
    "    dirname = [s for s in next(os.walk('.'))[1] if f'Run{runID}' in s][0]\n",
    "    savedir[runID] = f'{dirname}//{this_month}_MCE_fitting//'\n",
    "    Path(savedir[runID]).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "header = 'Hc[max(dH/dT)] (Oe), T[Hc[max(dH/dT)]] (K), \\\n",
    "Hc[max(d^2H/dT^2)] (Oe), T[Hc[max(d^2H/dT^2)]] (K)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltIDs = [2]\n",
    "dlabel = ['1st', '2nd']\n",
    "darr = {}\n",
    "# sr = -4\n",
    "for runID in pltIDs:#runIDs:\n",
    "    dirname = [s for s in next(os.walk('.'))[1] if f'Run{runID}' in s][0]\n",
    "    plt.figure()\n",
    "    susr = usr[runID][-1:]\n",
    "    for sr in sweeprates[runID][abs(sweeprates[runID])==susr]:#[-susr, susr]:#\n",
    "#     plt.plot(H[runID][sr], d1T[runID][sr])\n",
    "#     plt.plot(H[runID][sr], d2T[runID][sr]*1e2)\n",
    "#     plt.ylim(np.arange(-1, 2, 2)*1e-4)\n",
    "        plt.plot(H[runID][sr], T[runID][sr], '.', label=f'{sr:+.1f} Oe/s')\n",
    "        for elidx, elmt in enumerate(['dHc', 'HTcd1', 'HTcd2']):\n",
    "            darr[elmt] = np.asarray(list(eval(f'{elmt}[runID][sr].values()')))\n",
    "#         pltHTcd2 = np.asarray(list(HTcd2[runID][sr].values()))\n",
    "            if elidx>0:\n",
    "                plt.errorbar(darr[elmt][:,0], darr[elmt][:,1], xerr=darr['dHc'], ls='', lw=2,\n",
    "                             marker='*', ms=9, label=f'max {dlabel[elidx-1]} der.')\n",
    "#         plt.plot(darr['HTcd2'][:,0], darr['HTcd2'][:,1], marker='*', ms=9, label='max 2nd der.')\n",
    "#         plt.ylim(bottom=.4)\n",
    "        plt.xlabel(xlabel_mce)\n",
    "        plt.ylabel(ylabel_mce)\n",
    "        plt.title(f'Smoothed MCE traces run #{runID} - {run_date[runID]}')\n",
    "        plt.legend(loc='upper left')\n",
    "        \n",
    "        # Save data\n",
    "#         saveHcData = f'{savedir[runID]}{str(today)}_run{runID}_sweeprate{int(sr):+d}Oeps_Hc'\n",
    "#         data = np.vstack([pltHTcd1[:,0]*rescaling, pltHTcd1[:,1],\n",
    "#                           pltHTcd2[:,0]*rescaling, pltHTcd2[:,1]]).T\n",
    "#         np.savetxt(f'{saveHcData}.csv', data, fmt='%.3e', delimiter=',', header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveHcFig = f'{savedir[runID]}{str(today)}_run{runID}_sweeprate+-{int(susr)}Oeps_Hc'\n",
    "# plt.savefig(f'{saveHcname}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe containing all values of critical field \n",
    "as extracted from both the first and second derivatives of the MCE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTcdf = {}\n",
    "\n",
    "for runID in HTcd1.keys():\n",
    "    HTcdf[runID] = {}\n",
    "    for sr in HTcd1[runID].keys():\n",
    "        # Create a dataframe for each value of sweeprate in a given run\n",
    "        for Tb in HTcd1[runID][sr].keys():\n",
    "            HTcdf[runID][sr] = pd.concat([pd.DataFrame(HTcd1[runID][sr]).T, \n",
    "                                          pd.DataFrame(HTcd2[runID][sr]).T,\n",
    "                                          pd.Series(dHc[runID][sr])], axis=1)\n",
    "            HTcdf[runID][sr].columns = ['Hcd1_raw', 'Tcd1', 'Hcd2_raw', 'Tcd2', 'dHc']\n",
    "#             HTcdf[runID][sr]['dHc'] = abs(HTcdf[runID][sr]['Hcd1']-HTcdf[runID][sr]['Hcd2_raw'])\n",
    "            HTcdf[runID][sr]['Sweeprate'] = sr\n",
    "            HTcdf[runID][sr]['Run #'] = runID\n",
    "            HTcdf[runID][sr]['Date'] = run_date[runID]\n",
    "            HTcdf[runID][sr].reset_index(inplace=True)\n",
    "            HTcdf[runID][sr].rename(columns={'index':'Tbath'}, inplace=True)\n",
    "    \n",
    "    # For a given run, concatenate dataframes at all sweeprates together\n",
    "    HTcdf[runID]['all'] = pd.concat([HTcdf[runID][sr] for sr in HTcdf[runID].keys()], ignore_index=True)\n",
    "\n",
    "# Concatenate dataframes of all runs together\n",
    "HTcdf['all'] = pd.concat([HTcdf[runID]['all'] for runID in HTcdf.keys()], ignore_index=True)\n",
    "\n",
    "# Rescale values of critical field measured when sweeping field downwards\n",
    "for der_idx in range(1,3):\n",
    "    HTcdf['all'][f'Hcd{der_idx}'] = np.where(HTcdf['all']['Sweeprate']<0, \n",
    "                                             HTcdf['all'][f'Hcd{der_idx}_raw']*rescaling, \n",
    "                                             HTcdf['all'][f'Hcd{der_idx}_raw'])\n",
    "\n",
    "# Rearrange columns \n",
    "cols = HTcdf['all'].columns.tolist()\n",
    "cols = cols[:2] + cols[-2:-1] + cols[2:4] + cols[-1:] + cols[4:-2]\n",
    "HTcdf['all'] = HTcdf['all'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot critical field at each temperature vs sweeprate\n",
    "Hc_sel = {}\n",
    "xhc = {}\n",
    "dyhc = {}\n",
    "yhc = {}\n",
    "\n",
    "# %matplotlib \n",
    "plt.figure()\n",
    "for Tb in np.unique(HTcdf['all']['Tbath'])[:]:\n",
    "    # Hc_sel['upsweep'] = np.logical_and(HTcdf['all']['Tbath']==Tb, HTcdf['all']['Sweeprate']>0)\n",
    "    # Hc_sel['downsweep'] = np.logical_and(HTcdf['all']['Tbath']==Tb, HTcdf['all']['Sweeprate']<0)\n",
    "    Hc_sel['upsweep'] = HTcdf['all']['Tbath']==Tb\n",
    "\n",
    "    for sweep in ['upsweep']:\n",
    "        xhc[sweep] = HTcdf['all'][Hc_sel[sweep]]['Sweeprate']\n",
    "        yhc[sweep] = HTcdf['all'][Hc_sel[sweep]]['Hcd2']\n",
    "        dyhc[sweep] = HTcdf['all'][Hc_sel[sweep]]['dHc']\n",
    "\n",
    "    plt.errorbar(xhc['upsweep'], yhc['upsweep'], yerr=dyhc['upsweep'], fmt='.', label=f'{Tb}')\n",
    "    # plt.errorbar(xhc['upsweep'], HTcdf['all'][Hc_sel[sweep]]['Hcd2_raw'], yerr=dyhc['upsweep'], fmt='.')\n",
    "    # plt.errorbar(xhc['downsweep'], yhc['downsweep'], yerr=dyhc['downsweep'], fmt='.')\n",
    "    plt.legend(loc='lower left', ncol=2, title='$T_\\mathrm{{bath}}$ (K)')\n",
    "    plt.xlabel('Sweeprate (Oe/s)')\n",
    "    plt.ylabel('Critical field (Oe)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to csv file\n",
    "saveHcData = f'./Extracted_critical_fields/{str(today)}_Hc_all_runs.csv'\n",
    "# HTcdf['all'].to_csv(saveHcData, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert string of units in 2nd line of csv file\n",
    "Hc_units = 'K,Oe,Oe,K,Oe,Oe,K,Oe,Oe/s,,\\n'# units corresponding to HTcdf['all'].columns\n",
    "try:\n",
    "    with open(saveHcData, 'r+') as f:\n",
    "        contents = f.readlines()# store file contents\n",
    "        contents.insert(1, Hc_units)# insert string between first and second lines\n",
    "        f.seek(0)# reset cursor to begining of file\n",
    "    #     f.writelines(contents)# rewrite file with new contents\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do as of 2021-01-28:\n",
    "* Fit with Rafael's predictions\n",
    "* Repeat for sample measured in DR\n",
    "    \n",
    "Notes:\n",
    "* For the case of the 'needle-shaped samples, simulated curves have their upturns coincide with those of the data when determining Hc from second derivatives of MCE data, both for up- and downsweep; confirm with data on sample measured in DR\n",
    "* After rescaling Hc(downsweep) with mean(Hin/Hext), the critical fields of the up- and downsweeps agree well at all temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate MCE curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function for debugging\n",
    "def iter_callback(params, iter_num, resid, H, data=None, trace=None):\n",
    "    print(iter_num, params['ODE_prefactor'].value)\n",
    "    \n",
    "class FakeLMFitResult:\n",
    "    \"\"\"\n",
    "    Dummy class that has an attribute 'params', similar to the result of a fit with lmfit.\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        self.params = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute variables required to simulate MCE curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xfit_mce(H, T, Tpuck, Tb, htcd1, htcd2, fitRange, mfd, srpm, us, utmfd, fit=False):\n",
    "    d = {}\n",
    "    Hc = {}\n",
    "    for elmt in ['Hc', 'Hf', 'Tf', 'Hffit', 'Tffit', 'Tbath', 'prms', 'trace_str']:\n",
    "        exec(f'{elmt} = []', d)\n",
    "    for idx, sr in enumerate(srpm):#\n",
    "        # select data to analyze; note: solve_ivp fails for H<1 Oe (H/Hc0<2e-3)\n",
    "        dataFilter = np.logical_and(np.logical_and(H[sr]>10, H[sr]<9.99e3),\n",
    "                                    np.round(Tpuck[sr], 1)==Tb)\n",
    "        if len(T[sr][dataFilter])<100: continue\n",
    "        d['Hf'].append(H[sr][dataFilter])\n",
    "        d['Tf'].append(T[sr][dataFilter])\n",
    "        d['Tbath'].append(mce.bath_temp([d['Tf'][-1][d['Hf'][-1]<4e3]]))\n",
    "        if d['Tbath'] is None: continue\n",
    "        Hf_filter = np.logical_and(d['Hf'][-1]>fitRange[0], d['Hf'][-1]<fitRange[1])\n",
    "        d['Hffit'].append(d['Hf'][-1][Hf_filter])\n",
    "        d['Tffit'].append(d['Tf'][-1][Hf_filter])\n",
    "        if len(d['Hffit'][-1])<50: continue\n",
    "        if sr>0: \n",
    "            d['trace_str'].append('upsweep')\n",
    "            Hc[sr] = htcd2[sr][Tb][0]\n",
    "        else: \n",
    "            d['trace_str'].append('downsweep')\n",
    "            Hc[sr] = htcd2[sr][Tb][0]\n",
    "\n",
    "        \n",
    "    if len(d['Hf'])==0: return None\n",
    "    Tbm = np.mean(d['Tbath'])\n",
    "    # Find MFD with temperature closest to Tbath\n",
    "    Tbmfd, _ = closest_mfd_values(Tbm, utmfd)\n",
    "    Hc0 = 5e3\n",
    "    mfd_key = (Hc0, Tbmfd[0])\n",
    "    # Compute the number of datapoints in the range of the corresponding mfd\n",
    "    hnpoints = abs(abs(d['Hf'][0]/Hc0-1).idxmin() - abs(d['Hf'][0]/Hc0-(1-mfd[mfd_key]['rel_width'])).idxmin())\n",
    "    # Compute the corresponding histogram\n",
    "    mfd[mfd_key]['hc'], _, _ = mfd_histogram(mfd[mfd_key]['data'], \n",
    "                                             mfd_key[0], nbins=hnpoints)\n",
    "\n",
    "    for idx, sr in enumerate(srpm):#\n",
    "        # Define fit parameters\n",
    "        d['prms'].append(mce.mce_parameters(Hc=Hc[sr], sweeprate=us, \n",
    "                                            kappa=.05*np.sqrt(Tbm), \n",
    "                                            Tc0=2.2, Tbath=Tbm))\n",
    "    if fit is True:\n",
    "        out = minimize(mce.xmce_residual, d['prms'][-1], method='least_squares',\n",
    "                       args=(d['Hffit'], d['Tffit'], d['trace_str']),\n",
    "                       kws={'mfd_hc':mfd[mfd_key]['hc'], 'Tbath':d['Tbath']}, \n",
    "                       iter_cb=None)#, **{'ftol':1e-15})#, max_nfev=100)#)\n",
    "    else:\n",
    "        out = None\n",
    "    \n",
    "    return d['Hf'], d['Tf'], d['trace_str'], mfd_key, out, d['prms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# %matplotlib \n",
    "runID = 2\n",
    "for dico in [mce_fit, mce_sim]:\n",
    "    if runID not in dico.keys():\n",
    "        dico[runID] = {}\n",
    "\n",
    "dirname = [s for s in next(os.walk('.'))[1] if f'Run{runID}' in s][0]\n",
    "fitRange = {2:[4e3, 9e3], 3:[4e3, 6e3], 4:[4e3, 6e3], 5:[4e3, 8e3], 7:[4e3, 6.3e3]}\n",
    "# prm_rng = {2:(range(0,3,2), range(1,len(utbath[2]))), 3:None}\n",
    "\n",
    "for us in usr[runID][-1:]:\n",
    "    lgd_labels = []\n",
    "    plt.figure(num=int(us))\n",
    "    srpm = sweeprates[runID][abs(sweeprates[runID])==us]\n",
    "    for Tb in utbath[runID][1:]:\n",
    "        result = xfit_mce(H[runID], T[runID], Tpuck[runID], Tb, HTcd1[runID], HTcd2[runID],\n",
    "                          fitRange[runID], mfd, srpm, us, utmfd, fit=False)\n",
    "        if result is None: continue\n",
    "        Hf, Tf, strace, mfd_key, out, prms = result\n",
    "#         if out is not None:\n",
    "#             mce_fit[runID][us] = out\n",
    "\n",
    "        for idx, sr in enumerate(srpm):#\n",
    "            if out is None:\n",
    "                out = FakeLMFitResult(prms[idx])\n",
    "                print(f'{sr:+.0f} Oe/s\\t Tb = {Tb} K\\t Hc = {out.params[\"Hc\"].value:.0f} Oe\\\n",
    "                ODE prefactor = {out.params[\"ODE_prefactor\"].value:.3f}')\n",
    "                \n",
    "            mce_sim[runID][sr] = mce.mce_residual(out.params, Hf[idx], data=None, \n",
    "                                                  trace=strace[idx], mfd_hc=mfd[mfd_key]['hc'])\n",
    "            srlabel = f'{sr:+.0f} Oe/s'\n",
    "            out = None\n",
    "\n",
    "            # Plot results\n",
    "            plt.plot(Hf[idx], Tf[idx], '.', color=f'C{idx}', markersize=2,\n",
    "                     label=f'data {srlabel}' if srlabel not in lgd_labels else '_nolegend_')\n",
    "        #     plt.plot(Hf, mce_sim[sr], '-', color=f'C{idx}', label=f'fit {sr:+.1f} Oe/s')\n",
    "            # Plotting the fit with sorted arguments avoids lines connecting low and high Hf values\n",
    "            cutoff = 20\n",
    "            plt.plot(Hf[idx][:-cutoff].sort_values(), mce_sim[runID][sr][Hf[idx][:-cutoff].argsort()], \n",
    "                     '-', linewidth=1, color=f'C{idx}',\n",
    "                     label=f'fit {srlabel}' if srlabel not in lgd_labels else '_nolegend_')\n",
    "            # Keep track of legend labels to avoid duplicates\n",
    "            if srlabel not in lgd_labels:\n",
    "                lgd_labels.append(srlabel)\n",
    "    plt.title(f'MCE traces run #{runID} - {run_date[runID]}\\n\\\n",
    "    Up- and downsweep fits constrained together and include MFD')\n",
    "    plt.xlabel(xlabel_mce)\n",
    "    plt.ylabel(ylabel_mce)\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    # Save figure\n",
    "    savefitname = f'{dirname}//{this_month}_MCE_fitting//\\\n",
    "{str(today)}_run{runID}_sweeprate+-{int(us)}Oeps_+fits_noMFD.png'\n",
    "#     plt.savefig(savefitname, dpi=300)\n",
    "cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefitname\n",
    "plt.savefig(savefitname, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include Comsol MFD in MCE fits\n",
    "This section is for tests.\n",
    "Outdated as of 2021-01-28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test computation of MCE curve convolved with MFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runID = 2\n",
    "sr = 10\n",
    "Tb = 1.0\n",
    "dataSel = np.logical_and(np.logical_and(H[runID][sr]>10, H[runID][sr]<9.99e3),\n",
    "                         np.round(Tpuck[runID][sr], 1)==Tb)\n",
    "Hf = H[runID][sr][dataSel]\n",
    "Tf = T[runID][sr][dataSel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttest = Tb\n",
    "closest2T, Tweights = closest_mfd_values(Ttest, utmfd)\n",
    "print(Ttest, closest2T, Tweights)\n",
    "Htest = Hf[200]\n",
    "closest2H, Hweights = closest_mfd_values(Htest, uhext)\n",
    "print(Htest, closest2H, Hweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update mfd_key with the closest temperature\n",
    "mfd_key = (5e3, closest2T[0])\n",
    "\n",
    "# Compute the number of datapoints in the range of the corresponding mfd\n",
    "hnpoints = abs(Hf/Hc-1).idxmin() - abs(Hf/Hc-(1-mfd[mfd_key]['rel_width'])).idxmin()\n",
    "\n",
    "# Compute the corresponding histogram\n",
    "mfd[mfd_key]['hc'], mfd[mfd_key]['binCenters'], mfd[mfd_key]['binWidths'] = \\\n",
    "mfd_histogram(mfd[mfd_key]['data'], Hext, nbins=hnpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prms = mce.mce_parameters(Hc0=Hc0, sweeprate=-sr, Tc0=Tc0, Tbath=Tb)\n",
    "Tcalc = mce.mce_residual(prms, Hf, trace='downsweep')\n",
    "Tcalc[int(len(Tcalc)/2)-50:int(len(Tcalc)/2)+50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "# Update mfd_key to use the mfd closest to the critical field (H ~ 5 kOe)\n",
    "Tout = convolve(Tcalc, mfd[mfd_key]['hc'], mode='same')/sum(mfd[mfd_key]['hc'])\n",
    "# Check that the convolution acts on a wide enough range of magnetic fields. \n",
    "# Might require some kind of adaptability of its width...\n",
    "# At the very least, make sure the width is correct around the critical field\n",
    "Tout[int(len(Tout)/2)-50:int(len(Tout)/2)+50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convolution of magnetic field with MFD\n",
    "# fig, ax = plt.subplots()\n",
    "plt.plot(Hf, Tcalc)\n",
    "plt.plot(Hf, Tout)\n",
    "cursor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
